{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_management.dataset import AbstractCxrDataset\n",
    "import logging\n",
    "import re\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.utils.time import measure_time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CxrEvaluationCLIP(AbstractCxrDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_path: str,\n",
    "        img_dir: str,\n",
    "        testing: bool = False,\n",
    "    ):\n",
    "        import time\n",
    "\n",
    "        st_time = time.time()\n",
    "        self._img_dir = Path(img_dir)\n",
    "        self._labels = pd.read_csv(label_path, dtype=\"str\")\n",
    "        self._label_names = list(self._labels.columns[6:])\n",
    "        en_time = time.time()\n",
    "        print(f\"Time to load labels: {en_time - st_time}\")\n",
    "\n",
    "        st_time = time.time()\n",
    "        self._img_paths = self._create_img_paths()\n",
    "        en_time = time.time()\n",
    "        print(f\"Time to load images: {en_time - st_time}\")\n",
    "\n",
    "        st_time = time.time()\n",
    "        self._transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToPILImage(),\n",
    "                torchvision.transforms.Resize(224),\n",
    "                torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(mean=(0.485,), std=(0.229,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._tokenizer = self._create_tokenizer()\n",
    "        en_time = time.time()\n",
    "        print(f\"Time to load tokenizer: {en_time - st_time}\")\n",
    "\n",
    "    def _create_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"openai/clip-vit-base-patch32\",\n",
    "            truncation_side=\"left\",\n",
    "            padding_side=\"right\",\n",
    "            model_max_length=77,\n",
    "        )\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def _create_img_paths(self) -> List[Path]:\n",
    "        img_paths = []\n",
    "\n",
    "        def _create_path(row: pd.Series) -> Path:\n",
    "            part_id = self.extract_subject_part(row[\"subject_id\"])\n",
    "            subject_id = row[\"subject_id\"]\n",
    "            study_id = row[\"study_id\"]\n",
    "            img_id = row[\"dicom_id\"]\n",
    "            return (\n",
    "                self._img_dir\n",
    "                / f\"p{part_id}\"\n",
    "                / f\"p{subject_id}\"\n",
    "                / f\"s{study_id}\"\n",
    "                / f\"{img_id}.jpg\"\n",
    "            )\n",
    "\n",
    "        self._labels.apply(\n",
    "            lambda x: img_paths.append(_create_path(x)),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return img_paths\n",
    "\n",
    "    def _load_and_process_text(self, index: int) -> torch.Tensor:\n",
    "        # 26 pathologies -> tensor [26, (...)] with (...) is for each pathology\n",
    "        text = (\n",
    "            \"This patient has the following pathologies: \"\n",
    "            f\"{self._labels['pathology'][index]}\"\n",
    "        )\n",
    "\n",
    "        tokens = self._tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _load_and_process_img(self, img_path: Path) -> torch.Tensor:\n",
    "        img = super()._load_and_process_img(img_path)\n",
    "        img = self._transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self._load_and_process_img(self._img_paths[index])\n",
    "        txt = self._load_and_process_text(index)\n",
    "\n",
    "        return img, txt, int(self._labels[\"label\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "from myconfig import CFG\n",
    "from src.models.experiments import CxrVQA\n",
    "from src.utils import AvgMeter\n",
    "\n",
    "IS_TESTING = False\n",
    "\n",
    "\n",
    "def eval_collate_fn(batch):\n",
    "    imgs = [item[0] for item in batch]\n",
    "    print(imgs)\n",
    "    collated_data = dict()\n",
    "    for key in batch[0][1].keys():\n",
    "        data = [item[1][key] for item in batch]\n",
    "        collated_data[key] = torch.stack(data).squeeze()\n",
    "\n",
    "    stacked_imgs = torch.stack(imgs)\n",
    "    stacked_texts = BatchEncoding(data=collated_data)\n",
    "    stacked_labels = torch.tensor([item[2] for item in batch])\n",
    "    return stacked_imgs, stacked_texts, stacked_labels\n",
    "\n",
    "\n",
    "val_ds = CxrEvaluationCLIP(\n",
    "    label_path=\"/mnt/ssd1/CXR/data/classification-labels/test_train_long.csv\",\n",
    "    img_dir=\"/mnt/ssd1/CXR/data/cxr_dataset\",\n",
    "    testing=IS_TESTING,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.num_workers,\n",
    "    collate_fn=eval_collate_fn,\n",
    ")\n",
    "\n",
    "state_dict = torch.load(\"/mnt/ssd1/CXR/ckpts/best.pt\")\n",
    "model = CxrVQA()\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(CFG.device)\n",
    "\n",
    "n_classes = 26\n",
    "\n",
    "avg_auc = 0\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        imgs = batch[0].to(CFG.device)\n",
    "        texts = batch[1].to(CFG.device)\n",
    "        loss = model(imgs, texts)\n",
    "\n",
    "        count = len(batch[0])\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.losses.cross_entropy import CELoss\n",
    "\n",
    "\n",
    "tqdm_object = tqdm(val_dataloader, total=len(val_dataloader))\n",
    "\n",
    "criterion = CELoss()\n",
    "\n",
    "for batch in tqdm_object:\n",
    "    imgs = batch[0].to(CFG.device)\n",
    "    texts = batch[1].to(CFG.device)\n",
    "    sim = model.calcluate_similarity(imgs, texts)\n",
    "    loss = criterion(sim, batch[2].to(CFG.device))\n",
    "    count = len(batch[0])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds._tokenizer.batch_decode(sequences=val_ds.__getitem__(0)[1][\"input_ids\"].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.__getitem__(0)[1][\"input_ids\"].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(CFG.epochs):\n",
    "#     print(f\"Epoch: {epoch + 1}\")\n",
    "\n",
    "#     tqdm_object = tqdm(val_dataloader, total=len(val_dataloader))\n",
    "#     for batch in tqdm_object:\n",
    "#         imgs = batch[0].to(CFG.device)\n",
    "#         texts = batch[1].to(CFG.device)\n",
    "#         sim = model.calcluate_similarity(imgs, texts)\n",
    "\n",
    "#         count = len(batch[0])\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         valid_loss = valid_epoch(model, val_dataloader)\n",
    "#         print(f\"Valid Loss: {valid_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
